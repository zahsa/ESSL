{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "def get_algorithm(params_path):\n",
    "    with open(params_path, \"r\") as f:\n",
    "        params = []\n",
    "        for i, l in enumerate(f.readlines()):\n",
    "            params.append(l)\n",
    "    # get params\n",
    "    ssl = [j.split(\" \")[1].strip(\"\\n\") for j in params if \"ssl_task\" in j][0]\n",
    "    return ssl\n",
    "\n",
    "def get_best_downstream_loss(tb_dir):\n",
    "    ea = event_accumulator.EventAccumulator(tb_dir)\n",
    "    ea.Reload()\n",
    "    try:  \n",
    "        loss = ea.Scalars('best_downstream_loss__gen_9')[-1].value\n",
    "        return loss\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "exps =[f\"exp6_{i}\" for i in range(4)]+ [f\"exp8_{i}\" for i in range(4, 8)] + \\\n",
    "        [f\"exp10_{i}\" for i in range(4)] + [f\"exp11_{i}\" for i in range(4)]\n",
    "\n",
    "# get all experiment paths from the cc directory and bd directory (nothing in bd)\n",
    "cc_p = Path('/home/noah/ESSL/cc_experiments/')\n",
    "bd_p = Path('/home/noah/ESSL/exps/iteration4/')\n",
    "cc_dirs = [str(p_i) for p_i in list(cc_p.glob('**'))]\n",
    "bd_dirs = [str(p_i) for p_i in list(bd_p.glob('**'))]\n",
    "dirs = cc_dirs + bd_dirs\n",
    "# collect all experiments\n",
    "csv_map = []\n",
    "for e in exps: \n",
    "    e_dirs = [p_i for p_i in dirs if e == p_i.split(\"/\")[-1]]\n",
    "    csv_map_i = []\n",
    "    for e_dir in e_dirs:\n",
    "        # check that experiments actually completed\n",
    "        model_dirs = set([os.path.dirname(p) for p in glob.glob(os.path.join(e_dir, \"**/models\"))])\n",
    "        plot_dirs = set([os.path.dirname(p) for p in glob.glob(os.path.join(e_dir, \"**/plots\"))])\n",
    "        tbs = set([os.path.dirname(p) for p in glob.glob(os.path.join(e_dir, \"**/tensorboard\"))])\n",
    "        finished_exps = model_dirs.intersection(plot_dirs)\n",
    "        finished_exps_tbs = finished_exps.intersection(tbs)\n",
    "        if len(finished_exps) != len(finished_exps_tbs):\n",
    "            continue\n",
    "        # add comleted exps to csv_map_i\n",
    "        for f_e in finished_exps:\n",
    "            csv_map_i.append([e, os.path.basename(f_e), e_dir, f_e])\n",
    "    # append all results form exps to csv_map\n",
    "    csv_map+=sorted(csv_map_i, key=lambda x: str(x[1]))\n",
    "# create df for csv_map\n",
    "columns = [\"exp\", \"seed\", \"exp_dir\", \"seed_dir\"]\n",
    "df = pd.DataFrame(csv_map, columns=columns)\n",
    "df = df.drop_duplicates([\"exp\", \"seed\"],keep= 'last')\n",
    "df.to_csv(\"/home/noah/ESSL/PAPER/results/all_exps_map.csv\")\n",
    "pop_vals_df = []\n",
    "for exp in df[\"exp\"].unique():\n",
    "    exp_df = df[df[\"exp\"] == exp]\n",
    "    pop_vals = []\n",
    "    for _, row in exp_df.iterrows():\n",
    "        outcomes_dir = os.path.join(row[\"seed_dir\"], \"outcomes.json\")\n",
    "        algo = get_algorithm(os.path.join(row[\"seed_dir\"], \"params.txt\"))\n",
    "        try:\n",
    "            tb = glob.glob(os.path.join(row[\"seed_dir\"], \"tensorboard/*/*.tfevents*\" ))[0]\n",
    "            best_loss = get_best_downstream_loss(tb)\n",
    "        except:\n",
    "            best_loss = None\n",
    "        with open(outcomes_dir, \"r\") as f:\n",
    "            outcomes = json.load(f)\n",
    "            pop_vals+=[[exp, algo, row[\"seed\"], i[0], i[1], best_loss] for i in outcomes[\"pop_vals\"]]\n",
    "    pop_vals_df+=pop_vals\n",
    "pop_vals_df    \n",
    "df = pd.DataFrame(pop_vals_df, columns = [\"exp\", \"algo\", \"seed\", \"generation\", \"fitness\", \"best_loss\"])\n",
    "BS = {'exp6': 32, 'exp8':256, 'exp10':256, 'exp11':32}\n",
    "df['batch size'] = df['exp'].apply(lambda x: BS[x.split('_')[0]])\n",
    "DS = {'exp6': 'cifar10', 'exp8':'cifar10', 'exp10':'svhn', 'exp11':'svhn'}\n",
    "df['data set']  = df['exp'].apply(lambda x: DS[x.split('_')[0]])\n",
    "df.to_csv(\"/home/noah/ESSL/PAPER/results/full_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2233825259.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_24277/2233825259.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    for _, df_ds_bs_al df_ds_bs.groupby(\"algo\"):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme()\n",
    "# groupby dataset\n",
    "for _, df_ds in df.groupby(\"data set\"):\n",
    "    for _, df_ds_bs in df_ds.groupby(\"batch size\"):\n",
    "        for _, df_ds_bs_al in df_ds_bs.groupby(\"algo\"):\n",
    "            display(df_ds_bs)\n",
    "            sns.boxplot(df_ds_bs, x = \"generation\", y=\"fitness\")\n",
    "            plt.show()\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "essl",
   "language": "python",
   "name": "essl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
