{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/.conda/envs/essl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from essl.datasets import Cifar10, SVHN\n",
    "from essl.backbones import largerCNN_backbone\n",
    "from essl.evaluate_downstream import finetune_model\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_algorithm(params_path):\n",
    "    with open(params_path, \"r\") as f:\n",
    "        params = []\n",
    "        for i, l in enumerate(f.readlines()):\n",
    "            params.append(l)\n",
    "    # get params\n",
    "    ssl = [j.split(\" \")[1].strip(\"\\n\") for j in params if \"ssl_task\" in j][0]\n",
    "    bs = [j.split(\" \")[1].strip(\"\\n\") for j in params if \"ssl_batch_size\" in j][0]\n",
    "    return ssl, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10\n",
    "b256_fs = glob.glob(\"/home/noah/ESSL/final_exps/optimization/exp8*/*\")\n",
    "b256 = [os.path.join(f, \"outcomes.json\") for f in b256_fs]\n",
    "b256_names = [get_algorithm(os.path.join(f, \"params.txt\")) for f in b256_fs]\n",
    "\n",
    "b32_fs = glob.glob(\"/home/noah/ESSL/final_exps/optimization/exp6*/*\")\n",
    "b32 = [os.path.join(f, \"outcomes.json\") for f in b32_fs]\n",
    "b32_names = [get_algorithm(os.path.join(f, \"params.txt\")) for f in b32_fs]\n",
    "\n",
    "# SVHN\n",
    "b256_fs_svhn = glob.glob(\"/home/noah/ESSL/final_exps/optimization/exp10*/*\")\n",
    "b256_svhn = [os.path.join(f, \"outcomes.json\") for f in b256_fs_svhn]\n",
    "b256_names_svhn = [get_algorithm(os.path.join(f, \"params.txt\")) for f in b256_fs_svhn]\n",
    "\n",
    "b32_fs_svhn = glob.glob(\"/home/noah/ESSL/final_exps/optimization/exp11*/*\")\n",
    "b32_svhn = [os.path.join(f, \"outcomes.json\") for f in b32_fs_svhn]\n",
    "b32_names_svhn = [get_algorithm(os.path.join(f, \"params.txt\")) for f in b32_fs_svhn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(exp_path):\n",
    "    chromos = []\n",
    "    with open(exp_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        for fitness, chromo in zip(results[\"pop_vals\"], results[\"chromos\"]):\n",
    "            c = list(chain.from_iterable(chromo[1]))\n",
    "            c.append(fitness[1])\n",
    "            chromos.append(c)\n",
    "    columns = list(chain.from_iterable([[f\"aug{i}\", f\"op{i}\"] for i in range(1, 4)]))\n",
    "    columns.append(\"test acc\")\n",
    "    columns\n",
    "    df = pd.DataFrame(chromos, columns=columns)\n",
    "    \n",
    "    # create data in long format\n",
    "    ops = set(list(df[\"aug1\"]) + list(df[\"aug2\"]) +  list(df[\"aug3\"]))\n",
    "    indexes = {op:i for i, op in enumerate(ops)}\n",
    "    chromos_long = np.zeros([len(chromos), len(ops)+1])\n",
    "    for i, c in enumerate(chromos):\n",
    "        for aug, intensity in zip(c[:-2][::2], c[:-2][1::2]):\n",
    "            chromos_long[i][indexes[aug]] = intensity\n",
    "            chromos_long[i][-1] = c[-1]\n",
    "    columns_long = list(ops) + [\"fitness\"]\n",
    "    df_long = pd.DataFrame(chromos_long, columns = columns_long)\n",
    "    return df, df_long, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contrast', 0.9686110141915404, 'Solarize', 0.06499417612685054, 'VerticalFlip', 0.08094956837362144, 84.15]\n",
      "('NNCLR', '256')   84.15\n",
      "['Rotate', 22.0, 'TranslateY', 1.0, 'HorizontalFlip', 0.7158283743946523, 84.43]\n",
      "('SimSiam', '256')   84.43\n",
      "['ShearX', 0.09741248505133066, 'Brightness', 0.6905150754934473, 'Solarize', 0.18354873543483685, 84.1]\n",
      "('BYOL', '256')   84.1\n",
      "['ShearX', 0.0738254565954293, 'VerticalFlip', 0.1649378065796322, 'Rotate', -1.0, 84.23]\n",
      "('SwaV', '256')   84.23\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b256, b256_names):\n",
    "    df, df_long, ops = get_data(exp_path)\n",
    "    print(list(df.sort_values(by=\"test acc\").iloc[-1]))\n",
    "    print(exp_name, \" \", max(df_long['fitness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SimSiam', '32')   84.08\n",
      "('BYOL', '32')   84.12\n",
      "('NNCLR', '32')   84.18\n",
      "('SwaV', '32')   84.07\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b32, b32_names):\n",
    "    df, df_long, ops = get_data(exp_path)\n",
    "    print(exp_name, \" \", max(df_long['fitness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NNCLR', '256')   93.09695759065765\n",
      "('SwaV', '256')   93.20067609096495\n",
      "('BYOL', '256')   92.67440073755378\n",
      "('SimSiam', '256')   92.73970497848802\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b256_svhn, b256_names_svhn):\n",
    "    df, df_long, ops = get_data(exp_path)\n",
    "    print(exp_name, \" \", max(df_long['fitness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SwaV', '32')   93.29287031346036\n",
      "('NNCLR', '32')   92.76659496004918\n",
      "('SimSiam', '32')   93.4081130915796\n",
      "('BYOL', '32')   91.29917025199752\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b32_svhn, b32_names_svhn):\n",
    "    df, df_long, ops = get_data(exp_path)\n",
    "    print(exp_name, \" \", max(df_long['fitness']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top 2 accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top2(model_path, dataset):\n",
    "    backbone = largerCNN_backbone()\n",
    "    model = finetune_model(backbone=backbone.backbone, in_features=backbone.in_features, num_outputs=dataset.num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.cuda()\n",
    "    dataloader = DataLoader(dataset.test_data, batch_size=64,shuffle=False)\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        outputs = model(X)\n",
    "        vals, inds = torch.topk(outputs, k=2)\n",
    "        output_ohe = torch.nn.functional.one_hot(inds, dataset.num_classes)\n",
    "        output_ohe = torch.sum(output_ohe, dim=1)\n",
    "        y_ohe = torch.nn.functional.one_hot(y, dataset.num_classes) \n",
    "        correct = torch.sum(output_ohe * y_ohe)\n",
    "        total_correct += correct.item()\n",
    "        total+= X.shape[0]\n",
    "    return total_correct/total\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('SimSiam', '32')   0.9258\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('BYOL', '32')   0.932\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('NNCLR', '32')   0.9281\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('SwaV', '32')   0.9302\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b32, b32_names):\n",
    "    models = glob.glob(os.path.join(os.path.dirname(exp_path), \"models/*.pt\"))\n",
    "    if len(models) == 1:\n",
    "        model_path = models[0]\n",
    "    elif len(models) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        model_path = [m for m in models if \"downstream\" in m][0]\n",
    "    top2_acc = top2(model_path, dataset=Cifar10())\n",
    "    print(exp_name, \" \", top2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('NNCLR', '256')   0.9253\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('SimSiam', '256')   0.9323\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('BYOL', '256')   0.9291\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('SwaV', '256')   0.9219\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b256, b256_names):\n",
    "    models = glob.glob(os.path.join(os.path.dirname(exp_path), \"models/*.pt\"))\n",
    "    if len(models) == 1:\n",
    "        model_path = models[0]\n",
    "    elif len(models) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        model_path = [m for m in models if \"downstream\" in m][0]\n",
    "    top2_acc = top2(model_path, dataset=Cifar10())\n",
    "    print(exp_name, \" \", top2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "('SwaV', '32')   0.9657728948985863\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "('NNCLR', '32')   0.9575522433927474\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "('SimSiam', '32')   0.9582052858020897\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "('BYOL', '32')   0.9612015980331899\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b32_svhn, b32_names_svhn):\n",
    "    models = glob.glob(os.path.join(os.path.dirname(exp_path), \"models/*.pt\"))\n",
    "    if len(models) == 1:\n",
    "        model_path = models[0]\n",
    "    elif len(models) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        model_path = [m for m in models if \"downstream\" in m][0]\n",
    "    top2_acc = top2(model_path, dataset=SVHN())\n",
    "    print(exp_name, \" \", top2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "('NNCLR', '256')   0.9598570989551322\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "('SwaV', '256')   0.9615857406269207\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "('BYOL', '256')   0.9571681007990166\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "('SimSiam', '256')   0.965619237861094\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b256_svhn, b256_names_svhn):\n",
    "    models = glob.glob(os.path.join(os.path.dirname(exp_path), \"models/*.pt\"))\n",
    "    if len(models) == 1:\n",
    "        model_path = models[0]\n",
    "    elif len(models) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        model_path = [m for m in models if \"downstream\" in m][0]\n",
    "    top2_acc = top2(model_path, dataset=SVHN())\n",
    "    print(exp_name, \" \", top2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "essl",
   "language": "python",
   "name": "essl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
