{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/.conda/envs/essl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from essl.datasets import Cifar10, SVHN\n",
    "from essl.backbones import largerCNN_backbone\n",
    "from essl.evaluate_downstream import finetune_model\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top2(model_path, dataset):\n",
    "    backbone = largerCNN_backbone()\n",
    "    model = finetune_model(backbone=backbone.backbone, in_features=backbone.in_features, num_outputs=dataset.num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.cuda()\n",
    "    dataloader = DataLoader(dataset.test_data, batch_size=64,shuffle=False)\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        outputs = model(X)\n",
    "        vals, inds = torch.topk(outputs, k=2)\n",
    "        output_ohe = torch.nn.functional.one_hot(inds, dataset.num_classes)\n",
    "        output_ohe = torch.sum(output_ohe, dim=1)\n",
    "        y_ohe = torch.nn.functional.one_hot(y, dataset.num_classes) \n",
    "        correct = torch.sum(output_ohe * y_ohe)\n",
    "        total_correct += correct.item()\n",
    "        total+= X.shape[0]\n",
    "    return total_correct/total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b256 = [\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp8_4/4/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp8_5/2/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp8_6/1/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp8_7/7/outcomes.json\"\n",
    "]\n",
    "\n",
    "b256_names = [\"SwaV\", \"BYOL\", \"SimSiam\", \"NNCLR\"]\n",
    "\n",
    "b32 = [\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp6_1/6/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp6_2/8/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp6_3/3/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp6_0/3/outcomes.json\"\n",
    "]\n",
    "\n",
    "b32_names = [\"SwaV\", \"BYOL\", \"SimSiam\", \"NNCLR\"]\n",
    "\n",
    "b256SVHN = [\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp10_0/1/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp10_1/4/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp10_2/1/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp10_3/3/outcomes.json\"\n",
    "]\n",
    "\n",
    "b256_names = [\"SwaV\", \"BYOL\", \"SimSiam\", \"NNCLR\"]\n",
    "\n",
    "b32SVHN = [\n",
    "    #\"/home/noah/ESSL/final_exps/optimization/exp6_1/6/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp11_1/0/outcomes.json\",\n",
    "    #\"/home/noah/ESSL/final_exps/optimization/exp6_3/3/outcomes.json\",\n",
    "    \"/home/noah/ESSL/final_exps/optimization/exp11_3/1/outcomes.json\"\n",
    "]\n",
    "\n",
    "b32_names = [#\"SwaV\", \n",
    "             \"BYOL\", \n",
    "             #\"SimSiam\", \n",
    "             \"NNCLR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "BYOL   0.9612015980331899\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "NNCLR   0.9575522433927474\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b32SVHN, b32_names):\n",
    "    models = glob.glob(os.path.join(os.path.dirname(exp_path), \"models/*.pt\"))\n",
    "    if len(models) == 1:\n",
    "        model_path = models[0]\n",
    "    elif len(models) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        model_path = [m for m in models if \"downstream\" in m][0]\n",
    "    top2_acc = top2(model_path, dataset=SVHN())\n",
    "    print(exp_name, \" \", top2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "SwaV   0.9615857406269207\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "BYOL   0.9571681007990166\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "SimSiam   0.965619237861094\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: datasets/SVHN/train_32x32.mat\n",
      "NNCLR   0.9598570989551322\n"
     ]
    }
   ],
   "source": [
    "for exp_path, exp_name in zip(b256SVHN, b256_names):\n",
    "    models = glob.glob(os.path.join(os.path.dirname(exp_path), \"models/*.pt\"))\n",
    "    if len(models) == 1:\n",
    "        model_path = models[0]\n",
    "    elif len(models) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        model_path = [m for m in models if \"downstream\" in m][0]\n",
    "    top2_acc = top2(model_path, dataset=SVHN())\n",
    "    print(exp_name, \" \", top2_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "essl",
   "language": "python",
   "name": "essl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
